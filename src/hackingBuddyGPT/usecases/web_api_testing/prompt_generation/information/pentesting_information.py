from typing import Dict, List

from hackingBuddyGPT.usecases.web_api_testing.documentation.parsing import OpenAPISpecificationParser
from hackingBuddyGPT.usecases.web_api_testing.prompt_generation.information.prompt_information import (
    PromptPurpose,
)


class PenTestingInformation:
    def __init__(self, openapi_spec_parser: OpenAPISpecificationParser, username: str = "", password: str = "") -> None:
        """
        Initializes the PenTestingInformation with optional authentication credentials.

        Args:
            openapi_spec_parser (OpenAPISpecificationParser): An instance of OpenAPISpecificationParser.
            username (str, optional): Username for authentication, if necessary. Defaults to an empty string.
            password (str, optional): Password for authentication, if necessary. Defaults to an empty string.
        """
        # Set basic authentication details
        self.username = username
        self.password = password

        # Parse endpoints and their categorization from the given parser instance
        categorized_endpoints = openapi_spec_parser.classify_endpoints()

        # Assign schema and endpoint attributes directly from the parser methods
        self.schemas = openapi_spec_parser.get_schemas()
        self.endpoints = openapi_spec_parser.get_endpoints()

        # Assign categorized endpoint types to attributes
        self.assign_endpoint_categories(categorized_endpoints)

    def assign_endpoint_categories(self, categorized_endpoints):
        """
        Assign categorized endpoint types to instance attributes from given categorized endpoints dictionary.

        Args:
            categorized_endpoints (dict): A dictionary containing categorized endpoints.
        """
        self.resource_intensive_endpoint = categorized_endpoints.get('resource_intensive_endpoint')

        self.secure_action_endpoint = categorized_endpoints.get('secure_action_endpoint')
        self.role_access_endpoint = categorized_endpoints.get('role_access_endpoint')
        self.sensitive_data_endpoint = categorized_endpoints.get('sensitive_data_endpoint')
        self.sensitive_action_endpoint = categorized_endpoints.get('sensitive_action_endpoint')
        self.login_endpoint = categorized_endpoints.get('login_endpoint')
        self.auth_endpoint = categorized_endpoints.get('auth_endpoint')
        self.generate_iter_and_assign_current_endpoints(categorized_endpoints)

    def generate_iter_and_assign_current_endpoints(self, categorized_endpoints):
        for key in ['public_endpoint', 'protected_endpoint', 'refresh_endpoint']:
            endpoint_list = categorized_endpoints.get(key, [])
            if endpoint_list:
                setattr(self, f"{key}_iterator", iter(endpoint_list))
                setattr(self, f"current_{key}", next(getattr(self, f"{key}_iterator"), None))
            else:
                setattr(self, f"{key}_iterator", iter([]))
                setattr(self, f"current_{key}", None)

    def explore_steps(self) -> Dict[PromptPurpose, List[str]]:
        """
        Provides initial penetration testing steps for various purposes.

        Returns:
            dict: A dictionary where each key is a PromptPurpose and each value is a list of steps.
        """
        return {PromptPurpose.AUTHENTICATION: self.generate_authentication_prompts(),
                PromptPurpose.AUTHORIZATION: self.generate_authorization_prompts(),  # endpoint
                PromptPurpose.SPECIAL_AUTHENTICATION: self.generate_special_authentication(),
                PromptPurpose.INPUT_VALIDATION: self.generate_input_validation_prompts(),
                PromptPurpose.ERROR_HANDLING_INFORMATION_LEAKAGE: self.generate_error_handling_prompts(),
                PromptPurpose.SESSION_MANAGEMENT: self.generate_session_management_prompts(),
                PromptPurpose.CROSS_SITE_SCRIPTING: self.generate_xss_prompts(),
                PromptPurpose.CROSS_SITE_FORGERY: self.generate_csrf_prompts(),
                PromptPurpose.BUSINESS_LOGIC_VULNERABILITIES: self.generate_buisness_logic_vul_prompts(),
                PromptPurpose.RATE_LIMITING_THROTTLING: self.rate_limit_throttling(),
                PromptPurpose.SECURITY_MISCONFIGURATIONS: self.generate_security_misconfiguration_prompts(),
                PromptPurpose.LOGGING_MONITORING: self.generate_logging_monitoring_prompts()

                }

    def analyse_steps(self, response: str = "") -> Dict[PromptPurpose, List[str]]:
        """
        Provides prompts for analysis based on the provided response for various purposes using an LLM.

        Args:
            response (str, optional): The HTTP response to analyze. Default is an empty string.

        Returns:
            dict: A dictionary where each key is a PromptPurpose and each value is a list of prompts.
        """
        return {
            PromptPurpose.PARSING: [
                f"""Parse this response and extract the following details in JSON format: {{
                    "Status Code": "<status code>",
                    "Reason Phrase": "<reason phrase>",
                    "Headers": <headers as JSON>,
                    "Response Body": <body as JSON>
                    from this response: {response}

                }}"""
            ],
            PromptPurpose.ANALYSIS: [
                f"Given the following parsed HTTP response:\n{response}\n"
                "Analyze this response to determine in form of a RecordNote:\n"
                "1. Whether the status code is appropriate for this type of request.\n"
                "2. If the headers indicate proper security and rate-limiting practices.\n"
                "3. Whether the response body is correctly handled."
                # "Keep your analysis short."
            ],
            PromptPurpose.DOCUMENTATION: [
                f"Based on the analysis provided, document the findings of this API response validation in form of a RecordNote:\n{response}."
                # f" Keep your analysis short."
            ],
            PromptPurpose.REPORTING: [
                f"Based on the documented findings : {response}. Suggest any improvements or issues that should be reported to the API developers in form of a RecordNote."
                # f"Keep your analysis short."
            ],
        }

    def get_steps_of_phase(self, purpose):
        return self.explore_steps()[purpose]

    def next_testing_endpoint(self):
        self.current_public_endpoint = next(self.public_endpoint_iterator, None)
        self.current_protected_endpoint = next(self.protected_endpoint_iterator, None)
        self.current_refresh_endpoint = next(self.refresh_endpoint_iterator, None)

    def generate_authentication_prompts(self):
        prompts = []
        if self.current_protected_endpoint:
            prompts.append(
                [
                    [  # Check Access Without Authentication
                        f"Make a GET request to the protected endpoint {self.current_protected_endpoint} without including any authentication headers. "
                        f"Verify that the response status code is either 401 Unauthorized or 403 Forbidden, confirming that access is restricted. "
                        f"Ensure that no sensitive data is included in the response body."],
                    [  # Test Valid Authentication
                        f"First, send a POST request to the login endpoint {self.login_endpoint} with valid login credentials to obtain an authentication token. "
                        f"Then, use this token to send a GET request to {self.current_protected_endpoint}. "
                        f"Expect a 200 OK response, indicating successful authentication. "
                        f"Verify that the response includes the expected data and confirm that the token provides the correct access level."

                    ],
                ]

            )
            prompts.append([  # Test Expired or Invalid Tokens
                f"First, obtain a valid token by sending a POST request to the login endpoint {self.login_endpoint}. "
                f"Then, send a GET request to {self.current_protected_endpoint} using an expired, revoked, or otherwise invalid token. "
                f"Verify that the response status code is 401 Unauthorized, with an error message specifying invalid credentials."

            ])
            prompts.append(
                [  # Basic Authentication (if applicable)
                    f"First, obtain a token from the login endpoint {self.login_endpoint} using valid login credentials. "
                    f"Then, send a GET request to {self.current_protected_endpoint} using Basic Authentication. "
                    f"Expect a 200 OK response, confirming that access is granted. "
                    f"Next, send a similar request with incorrect credentials and verify a 401 Unauthorized status, indicating failed authentication."

                ])

            if self.current_refresh_endpoint:
                prompts.append([  # Test Token Refresh (if applicable)
                    [
                        f"Simulate an expired access token scenario if the API utilizes short-lived tokens with a refresh token mechanism. "

                        # Step 1: Attempt to access a protected endpoint with the expired token
                        f"Send a GET request to {self.current_refresh_endpoint} with the expired token in the Authorization header. "
                        f"Verify that the API responds with a 401 Unauthorized status, indicating the token has expired. "],

                    [  # Step 2: Request a new access token using the refresh token
                        f"Next, send a POST request to the token refresh endpoint {self.current_refresh_endpoint} with the valid refresh token in the request body or headers, "
                        f"depending on the API's token refresh requirements. "
                        f"Check if the API responds with a 200 OK status and includes a new access token in the response body."],

                    [  # Step 3: Verify that the old token is invalidated and the new token grants access
                        f"Finally, use the new access token to send a GET request to {self.current_protected_endpoint} again. "
                        f"Confirm that the API responds with a 200 OK status, indicating successful access with the refreshed token, "
                        f"and that the old expired token is no longer valid."]

                    # This prompt tests if the API correctly handles token expiration and issues a new token upon refresh,
                    # while ensuring that the expired token no longer provides access to protected resources.

                ])

        return prompts

    def generate_authorization_prompts(self):
        prompts = []
        if self.current_protected_endpoint:
            prompts.append(
                [  # Verify Role-Based Access Control (RBAC)
                    [  # Step 1: Verify Access for Admin Role
                        f"Send a GET request to {self.current_protected_endpoint} with an 'admin' role authentication token in the Authorization header. "
                        f"Expect a 200 OK response, indicating access to all resources. "
                        f"Verify that the response data includes complete access to the endpoint, reflecting the admin’s permissions."],

                    [  # Step 2: Verify Access for Regular User Role
                        f"Send a GET request to {self.current_protected_endpoint} with a 'user' role authentication token in the Authorization header. "
                        f"Expect a 200 OK response, but confirm that only authorized resources or data for a regular user are included in the response. "
                        f"Verify that any restricted admin-only resources are not accessible to the user role."],

                    [  # Step 3: Verify Access for Guest Role
                        f"Send a GET request to {self.current_protected_endpoint} with a 'guest' role authentication token (or without an authentication token if guests are unauthenticated). "
                        f"Expect either a 403 Forbidden or a 404 Not Found response if guests are not permitted to access the endpoint, "
                        f"or a 200 OK with limited data if some resources are publicly accessible to guests."]

                    # This prompt tests the RBAC implementation by confirming that each user role has the appropriate access level:
                    # - Admin has full access
                    # - User has restricted access
                    # - Guest has no or limited access.
                ]
            )
            prompts.append(

                [  # Access Control to Specific Resources
                    [  # Step 1: Accessing Resource with Owner's Credentials
                        f"Send a GET request to {self.current_protected_endpoint} using an authentication token for User A (the owner of the resource). "
                        f"Expect a 200 OK response, confirming that User A can access their own data. Verify that the response includes the resource data as expected."],

                    [  # Step 2: Attempting Access with Non-Owner's Credentials
                        f"Send a GET request to {self.current_protected_endpoint} using an authentication token for User B (a different user without ownership of the resource). "
                        f"Expect a 403 Forbidden or 404 Not Found response, indicating that access is denied to resources not owned by the authenticated user."],

                    [  # Step 3: Attempting Access without Authentication (if applicable)
                        f"Send a GET request to {self.current_protected_endpoint} without any authentication token. "
                        f"Expect a 401 Unauthorized response, confirming that unauthenticated access is restricted to user-owned resources."]

                    # This prompt tests access control for resources that are user-owned by verifying:
                    # - Only the resource owner can access the data (User A)
                    # - Non-owners receive a restricted response (User B)
                    # - Unauthenticated requests are denied access (if applicable)
                ])

            prompts.append([  # Check Permissions for CRUD Operations

                # Create Operation:
                [
                    f"Send a POST request to {self.current_protected_endpoint} with valid data to create a new resource, using a user with creation permissions. Expect a 201 Created response if successful. "
                    f"Then, attempt to send a similar request with a user who lacks creation permissions, expecting a 403 Forbidden or similar error response."
                ],

                # Read Operation:
                [
                    f"Send a GET request to {self.current_protected_endpoint} with a user who has read permissions, verifying that the correct data is returned. "
                    f"Attempt the same request with a user who lacks read permissions, and verify that the response returns a 403 Forbidden or 404 Not Found status."
                ],

                # Update Operation:
                [
                    f"Send a PUT or PATCH request to {self.current_protected_endpoint} to update a resource using a user with update permissions. Expect the resource to be modified, indicated by a 200 OK or 204 No Content response. "
                    f"Then, repeat the request with a user who lacks update permissions, expecting a 403 Forbidden response."
                ],

                # Delete Operation:
                [
                    f"Send a DELETE request to {self.current_protected_endpoint} with a user who has delete permissions, verifying a successful deletion with a 200 OK or 204 No Content response. "
                    f"Then, attempt to delete the resource with a user who lacks delete permissions, expecting a 403 Forbidden response."
                ],

                # Verify Data Masking
                [
                    f"Send a GET request to {self.current_protected_endpoint} with a user who has restricted access, verifying that only permissible fields are included in the response. "
                    f"Confirm that sensitive data is either masked or excluded, ensuring that only authorized fields are visible to the user."
                ],

                # Access Token Scope Testing
                [  # Access Token Scope Testing
                    [  # Step 1: Testing Read-Only Scope for Data Retrieval
                        f"Send a GET request to {self.current_protected_endpoint} using a read-only access token. "
                        f"Verify that the response status is 200 OK, allowing data retrieval. Ensure that only read access is permitted, and no data modification is possible with this token scope."
                    ],
                    [  # Step 2: Attempting Write Operation with Read-Only Token
                        f"Attempt to modify data on {self.current_protected_endpoint} by sending a POST, PUT, or DELETE request using the same read-only access token. "
                        f"Expect a 403 Forbidden or similar response, confirming that the token scope prevents unauthorized write actions."],
                    [
                        # Step 3: Testing Full-Access Token (if applicable)
                        f"Send a POST request to {self.current_protected_endpoint} using a full-access token to verify that write actions are permitted. "
                        f"Expect a 200 OK or 201 Created response, confirming that the token with full-access scope allows write operations on this endpoint."]

                    # This prompt tests that:
                    # - A read-only access token permits data retrieval (GET request).
                    # - The same read-only token denies access to write operations (POST, PUT, DELETE requests).
                    # - A full-access token (if applicable) allows write actions, validating proper enforcement of token scopes.
                ]

            ])
        return prompts

    def generate_special_authentication(self):
        prompts = []
        if self.login_endpoint:
            prompts.append([  # Brute Force Protection
                [
                    # Step 1: Attempt Login with Invalid Credentials
                    f"Send repeated POST requests to the login endpoint {self.login_endpoint} using invalid credentials (e.g., incorrect password). "
                    f"Simulate a high rate of attempts (e.g., 10 requests within 1 minute) to trigger potential brute force protection mechanisms."

                    # This request tests if the API has rate-limiting or account lockout mechanisms in place for multiple failed login attempts.
                ],
                [
                    # Step 2: Observe API Response for Rate Limiting
                    f"After the repeated failed login attempts, observe if the API responds with a 429 Too Many Requests status, indicating rate limiting, "
                    f"or an alternative account lockout response, such as 403 Forbidden."
                    # This request verifies that the API enforces security measures by limiting the number of rapid failed login attempts.
                ]])
        if self.auth_endpoint:
            prompts.append(
                [

                    # Cross-Site Scripting Protection (for OAuth)
                    [
                        # Step 1: Alter Redirect URI in OAuth Request
                        f"Send an OAuth authentication request to the endpoint {self.auth_endpoint} with a modified redirect URI that differs from the registered URI. "
                        f"The redirect URI in the request should be altered to a different URL (e.g., https://malicious.site/redirect) to test if the API enforces a strict redirect policy."
                        # This request tests if the API strictly validates the redirect URI in OAuth flows, preventing open redirection vulnerabilities.
                    ],
                    [
                        # Step 2: Confirm Rejection of Unmatched Redirect URI
                        f"Verify that the API responds with an error status, such as 400 Bad Request or 403 Forbidden, indicating rejection of the unauthorized redirect URI."
                        # This request ensures the API does not permit unauthorized redirect URIs in OAuth requests, preventing potential XSS or redirection attacks.
                    ]]
            )
        if self.current_protected_endpoint:
            prompts.append([

                # Token Revocation
                [
                    # Step 1: Use Valid Token on Protected Endpoint
                    f"First, obtain a valid token by logging in at {self.login_endpoint}. "
                    f"Then, send a GET request to {self.current_protected_endpoint} with this token in the Authorization header. "
                    f"Verify a 200 OK response, indicating the token is active and valid."

                    # This request establishes that the token is currently valid before testing revocation.
                ],
                [
                    # Step 2: Simulate Token Revocation
                    f"After logging in at {self.login_endpoint} to obtain a valid token, simulate revocation by updating the system or using an API to revoke the token's permissions, marking it as inactive."
                    # This step involves either a manual revocation process or an API call to revoke the token, if available.
                ],
                [
                    # Step 3: Attempt Access with Revoked Token
                    f"Send another GET request to {self.current_protected_endpoint} using the same (now revoked) token in the Authorization header. "
                    f"Expect a 401 Unauthorized response, confirming that the API denies access for revoked tokens and enforces token invalidation effectively."
                    # This request tests that the API recognizes revoked tokens and prevents their reuse, ensuring security after revocation.
                ]
            ])  # protected end point needed

        return prompts

    def generate_input_validation_prompts(self):
        prompts = []
        if self.current_protected_endpoint:
            prompts = [
                [
                    # Test Valid Data Input
                    f"Send a POST request to the endpoint {self.current_public_endpoint} with data matching the valid schema {self.schemas}. "
                    f"Example payload: {{'field1': 'value1', 'field2': 123}}. "
                    f"Verify that the API responds with a 200 OK or 201 Created status, indicating it correctly processes valid data."
                    # This request tests that the API processes valid data successfully and as expected.
                ],
                [
                    # Test Invalid Data Input
                    f"Send a POST request to {self.current_public_endpoint} with data that violates the schema {self.schemas} (e.g., missing required fields or incorrect data types). "
                    f"Example invalid payload: {{'field1': 123, 'field2': 'invalid type'}}. "
                    f"Expect a 400 Bad Request status, indicating the API correctly identifies and rejects invalid data."
                    # This request tests the API’s response to invalid data, ensuring it properly rejects malformed input.
                ],
                [
                    # Test Edge Case Data Input
                    f"Send a POST request to {self.current_public_endpoint} with edge case values, such as boundary values, very large numbers, or empty strings based on schema {self.schemas}. "
                    f"Example edge case payload: {{'field1': '', 'field2': 999999999999}}. "
                    f"Verify that the API either processes these values or returns appropriate error messages."
                    # This request tests if the API can handle extreme values or edge cases without errors.
                ],
                [
                    # Test Missing Required Fields
                    f"Send a POST request to {self.current_public_endpoint} omitting required fields based on {self.schemas}. "
                    f"Example payload: {{'field1': 'value1'}} (missing 'field2'). "
                    f"Check if the API responds with a 400 Bad Request and specifies the missing fields."
                    # This request tests if the API enforces required fields and provides feedback on missing data.
                ],
                [
                    # Test Special Characters and Injection Attacks
                    f"Send a POST request to {self.current_public_endpoint} with potentially malicious data, such as SQL or XSS payloads in fields. "
                    f"Example payload: {{'field1': '<script>alert(1);</script>', 'field2': '1 OR 1=1'}}. "
                    f"Verify that the API safely processes the data without executing any code or injection."
                    # This request tests if the API sanitizes inputs to prevent injection attacks.
                ],
                [
                    # Test Incorrect Data Format
                    f"Send a POST request to {self.current_public_endpoint} with data in incorrect formats (e.g., string instead of integer, as defined in {self.schemas}). "
                    f"Example payload: {{'field1': 'string_instead_of_int'}}. "
                    f"Expect a 400 Bad Request status, confirming that the API rejects incorrect data formats."
                    # This request tests if the API validates data formats and rejects incorrect data types.
                ],
                [
                    # General Error Handling and Response Code Check
                    "Send POST requests with a range of invalid inputs to test the API's error handling consistency. "
                    "Example payloads: missing fields, incorrect types, and special characters. Confirm that the API uses standard HTTP status codes (e.g., 400 for bad requests) and includes informative error messages."
                    # This request tests if the API uses consistent HTTP codes and provides clear error messages for invalid inputs.
                ],
                [
                    # Test for Common Injection Vulnerabilities
                    f"Attempt to exploit common injection vulnerabilities by sending malicious input to {self.current_public_endpoint} (e.g., SQL injection in text fields). "
                    f"Example payload: {{'field1': \"'; DROP TABLE users; --\"}}. "
                    f"Verify the API rejects and sanitizes the inputs safely."
                    # This request tests the API’s resistance to injection attacks, ensuring input handling is secure.
                ],
                [
                    # Validate Data Sanitization and Security
                    f"Send a POST request with data containing special characters and potentially harmful inputs to {self.current_public_endpoint}. "
                    "Confirm that the API sanitizes and escapes all data properly, ensuring no vulnerabilities are exposed."
                    # This request checks that the API applies appropriate sanitization to user-supplied data across all fields.
                ]
            ]

        return prompts

    def generate_error_handling_prompts(self):
        prompts = []
        if self.current_public_endpoint:
            prompts = [
                [
                    # Test Error Handling with Invalid Input
                    f"Send a POST request to {self.current_public_endpoint} with deliberately invalid input to trigger an error. "
                    f"Example payload: {{'nonexistent_field': 'invalid_value'}}. "
                    f"Verify that the API responds with a 400 Bad Request status and includes an error message indicating why the input is invalid."
                    # This request tests if the API correctly handles invalid input with appropriate error messages and status codes.
                ],
                [
                    # Test Error Handling with Missing Required Fields
                    f"Send a POST request to {self.current_public_endpoint} omitting one or more required fields (e.g., only providing optional fields). "
                    f"Example payload: {{'optional_field': 'test_value'}}. "
                    f"Confirm that the API returns a 400 Bad Request status and provides a detailed error message listing the missing required fields."
                    # This request checks if the API enforces required fields and gives helpful feedback on missing data.
                ],
                [
                    # Examine Error Message Detail for Information Leakage
                    f"Send a POST request to {self.current_public_endpoint} with incorrect field types or values, such as {{'field': 123}} (if expecting a string). "
                    f"Check if the response reveals sensitive details in the error message or headers, such as stack traces or server information."
                    # This request tests if the API avoids leaking sensitive information through error messages or headers.
                ],
                [
                    # Test Error Response for Security Vulnerabilities
                    f"Send various malformed requests to {self.current_public_endpoint}, such as using invalid JSON format (e.g., {{'field': 'value'). "
                    f"Verify that responses are generic and do not expose internal structure or sensitive information."
                    # This request confirms that malformed requests are handled securely and don’t expose implementation details.
                ],
                [
                    # Assess Error Messages for Corrective Action Suggestions
                    f"Send a POST request to {self.current_public_endpoint} with missing required fields or incorrect data types. "
                    f"Review if the error message provides corrective suggestions, specifying the missing fields or expected data types."
                    # This request tests if the API offers actionable feedback to help users correct their requests.
                ],
                [
                    # Look for Information Leakage Vulnerabilities
                    "Send requests with unexpected input, like special characters or SQL-like strings, to the endpoint. "
                    "Inspect if the response contains server details, stack traces, or database error messages, which could indicate information leakage."
                    # This request verifies that the API is secure against accidental exposure of sensitive information in responses.
                ]
            ]

        return prompts

    def generate_session_management_prompts(self):
        prompts = []
        if self.current_public_endpoint and self.login_endpoint:
            prompts = [
                [  # Check if API Uses Session Management
                    f"Send a GET request to each of the API endpoints {self.current_public_endpoint} without authentication to check if any session token or cookie is issued in the response headers. "
                    f"Observe if session management is implemented by checking for session-related identifiers in both request and response headers."
                    # This prompt tests if the API uses session management by observing session tokens or cookies in responses after a GET request.
                ],
                [  # Examine Session Handling Mechanism for Session Fixation Vulnerability
                    f"Authenticate and obtain a session identifier (token or cookie) by sending a POST request to the login endpoint {self.login_endpoint}. "
                    f"Reuse the same session identifier across different sessions or user accounts by sending additional POST requests to {self.current_public_endpoint} with this session ID. "
                    f"Check if the session ID remains valid, which may indicate a session fixation vulnerability."
                    # This prompt tests for session fixation by reusing a session identifier across multiple login attempts.
                ],
                [  # Test for Session Hijacking Vulnerability
                    f"Send a POST request to {self.login_endpoint} with valid credentials to obtain a session token or cookie. "
                    f"Then, simulate session hijacking by using this session identifier in a separate environment or device with a GET request to {self.current_public_endpoint} to check if access is granted without re-authentication."
                    # This prompt tests for session hijacking by attempting to reuse a session token from a different environment.
                ],
                [  # Check Session Timeout Settings
                    f"Start a session by sending a POST request to the login endpoint {self.login_endpoint} and obtaining a session identifier. "
                    f"Leave the session idle for the defined timeout period, then send a GET request to {self.current_public_endpoint} using the same session token or cookie. "
                    f"Verify if the session has expired and is rejected, indicating correct timeout handling."
                    # This prompt tests session timeout by verifying if a session expires after inactivity.
                ],
                [  # Test for Secure Session Cookie Settings
                    f"Send a POST request to the login endpoint {self.login_endpoint} to obtain a session cookie. "
                    f"Inspect the session token or cookie in the response headers to verify it includes secure attributes such as HttpOnly and Secure. "
                    f"Confirm that these settings prevent client-side access to the session cookie and enforce secure transmission."
                    # This prompt tests for secure session cookie attributes, such as HttpOnly and Secure, to protect against client-side access and enforce secure transmission.
                ]
            ]

        return prompts

    def generate_xss_prompts(self):
        prompts = []
        if self.current_public_endpoint:
            prompts = [
                [  # Test for XSS by Injecting Script into Text Fields
                    f"Send a POST request to {self.current_public_endpoint} where user input is accepted (e.g., comment or message fields). "
                    f"Include a payload with a basic script tag, such as <script>alert('XSS');</script>, in the request body to test if the API escapes or sanitizes it correctly. "
                    f"Check the response to ensure the script is not executed or reflected back."
                    # This prompt tests if the API properly sanitizes user inputs in text fields to prevent reflected XSS attacks.
                ],
                [  # Test for Persistent XSS
                    f"Send a POST request to {self.current_public_endpoint} to store data that might be reflected back to other users, such as profile descriptions or user comments. "
                    f"Inject a persistent XSS payload, like <script>alert('Persistent XSS');</script>, in the request body. "
                    f"Then, retrieve this data with a GET request to see if the script is executed, indicating a persistent XSS vulnerability."
                    # This prompt tests if the API prevents stored XSS by sanitizing inputs that might be displayed to other users.
                ],
                [  # Check for XSS in Query Parameters
                    f"Send a GET request to {self.current_public_endpoint} with a query parameter that includes a script payload, such as ?search=<script>alert('XSS');</script>. "
                    f"Observe if the script executes or is reflected in the response, indicating potential vulnerability."
                    # This prompt tests if the API properly handles and escapes query parameters to prevent reflected XSS.
                ],
                [  # Test for XSS in JSON Responses
                    f"Send a GET request to {self.current_public_endpoint} expecting a JSON response. Inject a payload in a parameter (e.g., ?param=<script>alert('XSS');</script>) and observe if the JSON response reflects this payload without escaping. "
                    f"Check if any part of the payload is executed, which would indicate improper escaping in JSON responses."
                    # This prompt tests if JSON responses are properly escaped, ensuring no XSS vulnerabilities when displaying data to users.
                ],
                [  # Inspect Headers and Metadata for XSS Protections
                    f"Send a GET request to {self.current_public_endpoint} and inspect the response headers (such as Content-Security-Policy and X-XSS-Protection) to verify if XSS protections are enabled. "
                    f"Ensure the API response headers contain adequate security settings to mitigate XSS risks."
                    # This prompt checks if appropriate security headers are set to protect against XSS vulnerabilities.
                ]
            ]

        return prompts

    def generate_csrf_prompts(self):
        prompts = []
        if self.sensitive_data_endpoint and self.sensitive_action_endpoint:
            prompts.append([
                [  # Test for CSRF Protection in Sensitive Actions
                    f"Send a POST request to {self.sensitive_action_endpoint} (e.g., account update, password change) without including any authentication tokens (e.g., CSRF token or session cookie). "
                    f"Verify if the API rejects the request with a 403 Forbidden status or similar error, indicating CSRF protection is in place."
                    # This prompt tests if the API protects sensitive actions by requiring proper authentication tokens.
                ],
                [  # Test for CSRF Token Requirement
                    f"Send a POST request to {self.sensitive_action_endpoint} with valid session cookies but without a CSRF token. "
                    f"Check if the API rejects the request, indicating that CSRF tokens are required for protected actions."
                    # This prompt verifies if the API requires a CSRF token, ensuring protection against CSRF attacks.
                ],
                [  # Test for CSRF Token Validation
                    f"Send a POST request to {self.sensitive_action_endpoint} with an incorrect or expired CSRF token, while including valid session cookies. "
                    f"Verify if the API rejects the request with an error indicating an invalid CSRF token."
                    # This prompt tests if the API validates the CSRF token, ensuring only valid tokens are accepted.
                ],
                [  # Test CSRF Protection in GET Requests
                    f"Send a GET request to {self.sensitive_data_endpoint} (if it involves sensitive data retrieval, e.g., viewing personal information). "
                    f"Check if the API requires a CSRF token or session cookie for the request, as some APIs also enforce CSRF protection on GET requests with sensitive data."
                    # This prompt tests if the API applies CSRF protection to GET requests that handle sensitive data.
                ],

            ])
        if self.login_endpoint:
            prompts.append([  # Check for SameSite Attribute on Cookies
                f"Send a POST request to {self.login_endpoint} to authenticate and obtain a session cookie. "
                f"Inspect the cookie attributes to confirm that the SameSite attribute is set to Strict or Lax, which helps prevent CSRF attacks by restricting cookie transmission."
                # This prompt checks if the API enforces CSRF protection by setting the SameSite attribute on cookies.
            ])
        return prompts

    def generate_buisness_logic_vul_prompts(self):
        prompts = []
        if self.current_public_endpoint:
            prompts.append(
                [  # Test for Unauthorized Access Through Business Logic
                    f"Send a POST request to {self.current_public_endpoint} with manipulated data, such as setting an unauthorized discount or modifying the item price. "
                    f"Verify if the API correctly restricts unauthorized actions or if it applies the manipulated changes."
                    # This prompt tests for flaws in the business logic that could allow unauthorized discounts or price manipulations.
                ])
        if self.sensitive_data_endpoint:
            prompts.append(
                [  # Test for Data Exposure via Business Logic Flaws
                    f"Send a GET request to {self.sensitive_data_endpoint} as a low-privilege user and attempt to retrieve sensitive information. "
                    f"Verify if the API prevents access to data beyond the user’s permission level."
                    # This prompt tests if the business logic prevents unauthorized access to sensitive user data.
                ])
        if self.role_access_endpoint:
            prompts.append(
                [  # Test for Manipulation of User Roles
                    f"Send a POST request to {self.role_access_endpoint} with an attempt to elevate a standard user’s role to an admin. "
                    f"Verify if the API restricts unauthorized role manipulation."
                    # This prompt tests if the API prevents unauthorized role escalation.
                ])
        return prompts

    def rate_limit_throttling(self):
        prompts = []
        if self.login_endpoint:
            prompts.append(
                [  # Test Rate-Limiting for Abuse Prevention
                    f"Send a large number of POST requests to {self.login_endpoint} within a short timeframe to check if the API rate-limits requests. "
                    f"Verify if the API responds with a 429 Too Many Requests status after reaching the limit."
                    # This prompt tests if the API has adequate rate-limiting to prevent abuse.
                ])
        if self.resource_intensive_endpoint:
            prompts.append(

                [  # Test Throttling for Denial-of-Service (DoS) Attack Prevention
                    f"Send continuous GET requests to {self.resource_intensive_endpoint} at high frequency and verify if the API throttles or blocks excessive requests to prevent DoS attacks."
                    # This prompt tests if the API prevents DoS attacks through request throttling.
                ])
        return prompts

    def generate_security_misconfiguration_prompts(self):
        prompts = []

        if self.current_public_endpoint:
            prompts.append(

                [  # Check for Security Misconfigurations in Headers
                    f"Send a GET request to {self.current_public_endpoint} and inspect response headers, such as X-Content-Type-Options, X-Frame-Options, and Content-Security-Policy. "
                    f"Verify if these headers are properly set to prevent security misconfigurations."
                    # This prompt tests if security headers are properly configured to prevent exposure to security risks.
                ])
        if self.secure_action_endpoint:
            prompts.append(
                [  # Inspect API for Debug or Error Information Exposure
                    f"Send an invalid POST request to {self.secure_action_endpoint} and check if detailed error messages or debug information is returned in the response. "
                    f"Verify that the response contains only minimal information to avoid exposing internal configuration details."
                    # This prompt tests if the API avoids exposing sensitive configuration or debug information.
                ])

    def generate_logging_monitoring_prompts(self):
        prompts = []
        if self.secure_action_endpoint:
            ''' [  # Test Logging for Normal Requests
                 f"Send a valid GET request to {self.endpoints['normal_activity']} and observe if the API logs the request details as expected. "
                 f"Verify that the request is recorded in logs, including timestamp, user ID, and endpoint accessed."
                 # This prompt tests if the API properly logs standard, valid requests.
             ],
             '''
            prompts.append(
                [  # Test Logging for Incorrect Requests
                    f"Send an invalid POST request to {self.secure_action_endpoint} and verify if the API logs the failed attempt. "
                    f"Check if details of the invalid request, including the error and user information, are recorded in the logs."
                    # This prompt tests if the API logs incorrect requests, including any errors encountered.
                ])
        if self.sensitive_data_endpoint:
            prompts.append(
                [  # Test Logging for Potentially Malicious Requests
                    f"Send a GET request to {self.sensitive_data_endpoint} with unusual parameters (e.g., SQL injection attempts) to simulate a malicious request. "
                    f"Check if the API logs the suspicious request with appropriate details and flags it for monitoring."
                    # This prompt tests if the API logs and monitors potentially malicious requests to help detect and respond to security threats.
                ])
        return prompts
